---
title: "DEanalysis"
---

<https://hbctraining.github.io/Intro-to-DGE/schedule/links-to-lessons.html>

```{r}
#Pakete werden aus Setup Skript geladen
source("../../org/setup_packages.R")

library(tidyverse)  
library(readxl)
library(janitor)
library(DESeq2)
library(tximport)
library(pheatmap)
library(RColorBrewer)
library(ggrepel)
library(cowplot)
library(DEGreport)
library(clusterProfiler)
library(DOSE)
library(org.Hs.eg.db)
library(pathview)
library(AnnotationHub)
library(ensembldb)
library(apeglm)
library(ashr)

sessionInfo()
```

```{r}
#| echo: false
#| warning: false
# Pfade
salmon <- "../../dat/salmon_files"
tx2g   <- "../../dat/info_files/tx2gene.tsv"
excel  <- "../../dat/info_files/CCGA_sequencing_NM_ventricular_dysfunction_RNAseq_2342.xlsx"

# Salmon-Files einlesen
files <- list.files(
  path      = salmon,
  full.names = TRUE,
  pattern   = "_quant\\.sf$"
)

# Saubere Namen f√ºr tximport
names(files) <- basename(files) %>%
  str_remove("_quant\\.sf$") %>%    # Endung entfernen
  str_remove("-L[0-9]+$")           # evtl. Library-Nummer entfernen

# Duplikate ausschlie√üen
stopifnot(length(files) > 1)
stopifnot(!anyDuplicated(names(files)))

# tx2gene einlesen
tx2gene <- read.delim(tx2g)
tx2gene[,1] <- sub("\\..*$", "", tx2gene[,1])  # Versionsnummer entfernen
tx2gene <- tx2gene[, c(1,2)]                    # Nur tx_id und gene_id

# tximport
txi_cache <- "../../res/cache/txi_salmon_tximport.rds"
dir.create(dirname(txi_cache), recursive = TRUE, showWarnings = FALSE)

if (file.exists(txi_cache)) {
  message("‚úÖ Lade tximport Cache: ", txi_cache)
  txi <- readRDS(txi_cache)
} else {
  message("‚è≥ Rechne tximport‚Ä¶")
  txi <- tximport(
    files,
    type = "salmon",
    tx2gene = tx2gene,
    countsFromAbundance = "lengthScaledTPM",
    ignoreTxVersion = TRUE
  )
  saveRDS(txi, txi_cache)
  message("üíæ Gespeichert: ", txi_cache)
}

# Sample-Namen in txi bereinigen
colnames(txi$counts)    <- str_remove(colnames(txi$counts), "_quant\\.sf$")
colnames(txi$abundance) <- str_remove(colnames(txi$abundance), "_quant\\.sf$")
colnames(txi$length)    <- str_remove(colnames(txi$length), "_quant\\.sf$")

# ‚îÄ‚îÄ‚îÄ 6Ô∏è‚É£ Metadata einlesen ‚îÄ‚îÄ‚îÄ
md <- read_excel(excel, sheet = "Metadata", skip = 1) %>%
  janitor::clean_names() %>%
  mutate(sample_id = str_remove(library_id, "-L[0-9]+$") %>% str_trim())

input <- read_excel(excel, sheet = 3, skip = 1) %>%
  janitor::clean_names() %>%
  mutate(
    valve_treated = if_else(str_starts(sample_name, regex("^t", TRUE)),
                            "tricuspid valve",
                            "aortic valve"),
    timepoint = str_extract(sample_name, "(?<=_).*"),
    pid       = str_extract(sample_name, "^[^_]+")
  ) %>%
  dplyr::select(external_id, valve_treated, timepoint, pid)

md <- left_join(md, input, by = "external_id")
stopifnot(all(!is.na(md$valve_treated)))

# ============================================================
# ‚úÖ DIAGNOSTIK: Counts-Samples vs. Metadata-Samples
# (sehr h√§ufige Fehlerquelle: IDs passen nicht 1:1)
# ============================================================
counts_samples <- colnames(txi$counts)

in_counts_not_meta <- setdiff(counts_samples, md$sample_id)
in_meta_not_counts <- setdiff(md$sample_id, counts_samples)

message("Samples in counts but not metadata: ", length(in_counts_not_meta))
message("Samples in metadata but not counts: ", length(in_meta_not_counts))

if (length(in_counts_not_meta) > 0) {
  message("‚Üí In counts, not in metadata:")
  print(in_counts_not_meta)
}

if (length(in_meta_not_counts) > 0) {
  message("‚Üí In metadata, not in counts:")
  print(in_meta_not_counts)
}

# Abbrechen, wenn es Mismatches gibt
stopifnot(length(in_counts_not_meta) == 0, length(in_meta_not_counts) == 0)


# Nur Aorten-Samples behalten
keep_aortic <- intersect(colnames(txi$counts),
                         md$sample_id[md$valve_treated == "aortic valve"])

stopifnot(length(keep_aortic) > 1)
stopifnot(!anyDuplicated(keep_aortic))

txi_aortic <- list(
  counts    = txi$counts[, keep_aortic, drop = FALSE],
  abundance = txi$abundance[, keep_aortic, drop = FALSE],
  length    = txi$length[, keep_aortic, drop = FALSE]
)


md_aortic <- md[md$sample_id %in% keep_aortic, ]

cat("Anzahl Aorten-Samples:", length(keep_aortic), "\n")
head(md_aortic)


```

```{r}
#| echo: false
#| warning: false

### Exploring RNA-seq count data

#Die Count-Verteilung einer einzelnen Probe ansehen
counts_aortic <- as.data.frame(txi_aortic$counts)

sample <- colnames(counts_aortic)[1]

ggplot(counts_aortic) +
  geom_histogram(aes(x = .data[[sample]]), bins = 200) +
  xlab("Raw expression counts") +
  ylab("Number of genes")

# Mean‚ÄìVariance Plot (RNA-seq count data) f√ºr Aorten-Counts

# 2) Mean und Varianz pro Gen (√ºber alle ausgew√§hlten Samples/Spalten)
mean_counts     <- apply(counts_aortic, 1, mean, na.rm = TRUE)  # '1' = zeilenweise (Gene)
variance_counts <- apply(counts_aortic, 1, var,  na.rm = TRUE)

# 3) Dataframe f√ºr ggplot
df_mv <- data.frame(
  mean_counts = mean_counts,
  variance_counts = variance_counts
)

# Gene mit mean/var <= 0 entfernen (log10 braucht > 0)
df_mv <- dplyr::filter(
  df_mv,
  mean_counts > 0,
  variance_counts > 0
)

# 4) Plot: log10-mean vs log10-variance, rote Linie = x=y
ggplot(df_mv) +
  geom_point(aes(x = mean_counts, y = variance_counts), alpha = 0.4) +
  scale_x_log10(limits = c(1, 1e9)) +
  scale_y_log10(limits = c(1, 1e9)) +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  labs(
    title = "Mean‚ÄìVariance Relationship (Aortic valve RNA-seq counts)",
    x = "Mean of counts per gene (log10)",
    y = "Variance of counts per gene (log10)"
  )
# Daten passen nicht zur Poission-Verteilung, weil die Varianz >> Mittelwert (Overdispersion). Besonders bei h√∂her exprimierten Genene ist die Varianz viel gr√∂√üer. Das rechtfertigt die Verwendung der Negativ-Binomial-Modelle von DESeq2

# ============================================================
# DESeq2: Normalisierung (Size-Factor / "median-of-ratios")
# Kontext: Salmon -> tximport -> DESeq2
# Ziel: Normalisierte Counts erzeugen (nur f√ºr Visualisierung/QC),
#       NICHT als Input f√ºr DE-Tests verwenden.
# ============================================================

# ------------------------------------------------------------
# VORAUSSETZUNGEN (existieren bereits im Workspace)
#   txi        : komplettes tximport-Objekt (aus tximport())
#   txi_aortic : gefiltertes tximport-Objekt f√ºr Aorten-Samples
#               (list mit counts/abundance/length)
#   md_aortic  : Metadata passend zu Aorten-Samples
#               (mind. sample_id, pid, timepoint)
# ------------------------------------------------------------

# ------------------------------------------------------------
# 1) Metadata f√ºr DESeq2 vorbereiten
#    - sample_id muss character sein (IDs)
#    - pid/timepoint als factor (Design-Variablen)
#    - rownames(meta) m√ºssen exakt die Sample-IDs sein
# ------------------------------------------------------------
meta <- md_aortic %>%
  mutate(
    sample_id = as.character(sample_id),
    pid       = factor(pid),        # Patient-ID
    timepoint = factor(timepoint)   
  ) %>%
  as.data.frame()

rownames(meta) <- meta$sample_id

# ------------------------------------------------------------
# 2) Sicherstellen, dass Counts-Spalten und Metadata-Zeilen matchen
#    DESeq2 verlangt:
#      colnames(counts) == rownames(colData)
#    Reihenfolge ist KRITISCH (sonst falsche Zuordnung)
# ------------------------------------------------------------

# Check: alle Count-Sample-IDs existieren in meta?
stopifnot(all(colnames(txi_aortic$counts) %in% rownames(meta)))

# Meta exakt in die Reihenfolge der Count-Spalten bringen
meta <- meta[match(colnames(txi_aortic$counts), rownames(meta)), , drop = FALSE]

# Check: jetzt exakt identisch und in gleicher Reihenfolge?
stopifnot(all(colnames(txi_aortic$counts) == rownames(meta)))

# ------------------------------------------------------------
# 3) WICHTIG: tximport-Metainfo erhalten
#    DESeqDataSetFromTximport() erwartet u.a. txi$countsFromAbundance
#    Wenn txi_aortic "manuell" als Liste gebaut wurde, fehlt das oft.
#    Wir kopieren deshalb den Wert aus dem originalen txi.
# ------------------------------------------------------------
txi_aortic$countsFromAbundance <- txi$countsFromAbundance

# ------------------------------------------------------------
# 4) DESeqDataSet erstellen
#    Design: ~ pid + timepoint
#    - pid modelliert patientenspezifische Baselines
#    - timepoint testet die systematische √Ñnderung √ºber die Zeit
# ------------------------------------------------------------
dds <- DESeqDataSetFromTximport(
  txi = txi_aortic,
  colData = meta,
  design = ~ pid + timepoint
)

# ------------------------------------------------------------
# 5) (Optional aber empfohlen) Prefiltering:
#    Entfernt Gene mit extrem niedrigen Counts in allen Samples,
#    verbessert Laufzeit und reduziert Noise.
# ------------------------------------------------------------
dds <- dds[rowSums(counts(dds)) >= 10, ]

# ------------------------------------------------------------
# 6) Normalisierung: Size Factors sch√§tzen
#    DESeq2 nutzt "median-of-ratios" (robust gegen Outlier/Geneffekte)
#    Hinweis: In der echten DE-Analyse macht DESeq() das automatisch.
# ------------------------------------------------------------
dds <- estimateSizeFactors(dds)

# Size factors inspizieren (sollten grob um 1 liegen; Unterschiede = Library size etc.)
sf <- sizeFactors(dds)
print(sf)

# ------------------------------------------------------------
# 7) Normalisierte Counts extrahieren
#    Diese Matrix ist sinnvoll f√ºr:
#      - PCA / Heatmaps / Plots
#    NICHT sinnvoll als Input f√ºr DESeq2-DE-Tests (die arbeiten mit Rohcounts + Modell)
# ------------------------------------------------------------
normalized_counts <- counts(dds, normalized = TRUE)

# ------------------------------------------------------------
# 8) Speichern
# ------------------------------------------------------------
write.table(
  normalized_counts,
  file = "../../res/normalized_counts_aortic_DESeq2.tsv",
  sep = "\t",
  quote = FALSE,
  col.names = NA
)
```

```{r}
### ============================================================
### Quality Control (QC) ‚Äì DESeq2 RNA-seq
### Hinweis: rlog() kann bei >=50 Samples lange dauern; vst() ist deutlich schneller.
### Daher: nur vst() verwenden (auch f√ºr PC3/PC4), kein rlog().
### ============================================================

# ------------------------------------------------------------
# 1) Varianz-stabilisierende Transformation (schnell f√ºr viele Samples)
# ------------------------------------------------------------
trans <- vst(dds, blind = TRUE)

# ------------------------------------------------------------
# 2) PCA ‚Äì schnelle √úbersicht (DESeq2-Default)
# ------------------------------------------------------------
plotPCA(trans, intgroup = "timepoint")

# ------------------------------------------------------------
# 3) PCA ‚Äì angepasstes ggplot (PC1 vs PC2)
# ------------------------------------------------------------
pca_df <- plotPCA(trans, intgroup = "timepoint", returnData = TRUE)
percentVar <- round(100 * attr(pca_df, "percentVar"))

ggplot(pca_df, aes(x = PC1, y = PC2, color = timepoint)) +
  geom_point(size = 3) +
  xlab(paste0("PC1: ", percentVar[1], "% Varianz")) +
  ylab(paste0("PC2: ", percentVar[2], "% Varianz")) +
  theme_minimal()

# ------------------------------------------------------------
# 4) PCA ‚Äì h√∂here Komponenten (PC3 vs PC4) mit vst-Matrix
# ------------------------------------------------------------
trans_mat <- assay(trans)        # Gene x Samples
pca <- prcomp(t(trans_mat))      # PCA auf Samples

df_pc <- cbind(
  as.data.frame(colData(dds)),
  as.data.frame(pca$x)
)

ggplot(df_pc, aes(x = PC3, y = PC4, color = timepoint)) +
  geom_point(size = 3) +
  theme_minimal()

##Die PCAs clustern eher nicht nach Timepoint, weil der Patienteneffekt > Timepoint-Effekt. Die Expression eines Patienten √ºber mehrere Zeitpunkte ist √§hnlicher als die Expression verschiedener Patienten √ºber einem Zeitpunkt.
# ------------------------------------------------------------
# 5) Hierarchisches Clustering ‚Äì Sample-Sample-Korrelation + Heatmap
# ------------------------------------------------------------
sample_cor <- cor(trans_mat)     # Samples x Samples

anno <- as.data.frame(colData(dds)[, "timepoint", drop = FALSE])
anno$timepoint <- factor(anno$timepoint)

anno <- anno[match(colnames(sample_cor), rownames(anno)), , drop = FALSE]
stopifnot(all(rownames(anno) == colnames(sample_cor)))

pheatmap(sample_cor,
         annotation_col = anno,
         show_colnames = FALSE,
         show_rownames = FALSE,
         breaks = seq(0.9, 1, length.out = 100) #damit werden feine Unterschiede                                                    etwas sichtbarer
         )
##Heatmap zeigt hohe Korrelationen, was f√ºr technisch saubere Daten spricht

```

```{r}
# ============================================================
# DESeq2 DE-Analyse "nur wenn n√∂tig" (Caching via RDS-Dateien)
# - Rechnet DESeq(dds) nur, wenn fitted dds noch nicht gespeichert ist
# - Berechnet/ speichert Result- und Shrinkage-Tabellen nur, wenn nicht vorhanden
# Voraussetzung: Objekt 'dds' (unfitted) existiert bereits in der Session
# ============================================================

# Speicherorte
cache_dir <- "../../res/cache"
if (!dir.exists(cache_dir)) dir.create(cache_dir, recursive = TRUE)

dds_cache_file <- file.path(cache_dir, "dds_DESeq_fitted.rds")

# 1) Fitted dds laden oder berechnen
if (file.exists(dds_cache_file)) {

  message("‚úÖ Lade bereits berechnetes DESeq2-Objekt: ", dds_cache_file)
  dds <- readRDS(dds_cache_file)

} else {

  message("‚è≥ Berechne DESeq2 (kann dauern)‚Ä¶")

  # sicherstellen, dass timepoint korrekt als Faktor vorliegt
  colData(dds)$timepoint <- factor(colData(dds)$timepoint, levels = c("A", "V1", "V2"))

  # Design setzen
  design(dds) <- ~ pid + timepoint

  # Prefiltering
  dds <- dds[rowSums(counts(dds)) >= 10, ]

  # DESeq laufen lassen
  dds <- DESeq(dds)

  # speichern
  saveRDS(dds, dds_cache_file)
  message("üíæ Gespeichert: ", dds_cache_file)
}

# ------------------------------------------------------------
# Helper: RDS-first caching (optional zus√§tzlich CSV-Export)
# ------------------------------------------------------------
run_or_load_rds <- function(rds_path, compute_expr, csv_path = NULL) {
  if (file.exists(rds_path)) {
    message("‚úÖ Lade: ", rds_path)
    return(readRDS(rds_path))
  }

  message("‚è≥ Berechne & speichere: ", rds_path)
  out <- eval(compute_expr)

  saveRDS(out, rds_path)

  if (!is.null(csv_path)) {
    # F√ºr Sharing/Excel etc. (Attributes gehen in CSV verloren ‚Äì ist okay)
    write.csv(as.data.frame(out), csv_path)
    message("üßæ Zus√§tzlich als CSV exportiert: ", csv_path)
  }

  out
}

# ------------------------------------------------------------
# 2) Raw Results (Wald-Tests) + RDS/CSV Cache
# ------------------------------------------------------------
alpha_level <- 0.05

res_dir <- "../../res"
if (!dir.exists(res_dir)) dir.create(res_dir, recursive = TRUE)

res_V1_vs_A_rds  <- file.path(cache_dir, "DESeq2_results_V1_vs_A.rds")
res_V2_vs_A_rds  <- file.path(cache_dir, "DESeq2_results_V2_vs_A.rds")
res_V2_vs_V1_rds <- file.path(cache_dir, "DESeq2_results_V2_vs_V1.rds")

res_V1_vs_A_csv  <- file.path(res_dir, "DESeq2_results_V1_vs_A.csv")
res_V2_vs_A_csv  <- file.path(res_dir, "DESeq2_results_V2_vs_A.csv")
res_V2_vs_V1_csv <- file.path(res_dir, "DESeq2_results_V2_vs_V1.csv")

res_V1_vs_A <- run_or_load_rds(
  rds_path = res_V1_vs_A_rds,
  csv_path = res_V1_vs_A_csv,
  compute_expr = quote(
    results(dds, contrast = c("timepoint", "V1", "A"), alpha = alpha_level)
  )
)

res_V2_vs_A <- run_or_load_rds(
  rds_path = res_V2_vs_A_rds,
  csv_path = res_V2_vs_A_csv,
  compute_expr = quote(
    results(dds, contrast = c("timepoint", "V2", "A"), alpha = alpha_level)
  )
)

res_V2_vs_V1 <- run_or_load_rds(
  rds_path = res_V2_vs_V1_rds,
  csv_path = res_V2_vs_V1_csv,
  compute_expr = quote(
    results(dds, contrast = c("timepoint", "V2", "V1"), alpha = alpha_level)
  )
)

# ------------------------------------------------------------
# 3) Shrinkage + RDS/CSV Cache
#    - apeglm via coef (f√ºr timepoint-Koeffizienten)
#    - ashr via contrast (geht gut f√ºr beliebige Kontraste)
#    - UND: res=... koppeln (Konsistenz)
# ------------------------------------------------------------
lfc_V1_vs_A_rds  <- file.path(cache_dir, "DESeq2_LFCshrink_V1_vs_A.rds")
lfc_V2_vs_A_rds  <- file.path(cache_dir, "DESeq2_LFCshrink_V2_vs_A.rds")
lfc_V2_vs_V1_rds <- file.path(cache_dir, "DESeq2_LFCshrink_V2_vs_V1.rds")

lfc_V1_vs_A_csv  <- file.path(res_dir, "DESeq2_LFCshrink_V1_vs_A.csv")
lfc_V2_vs_A_csv  <- file.path(res_dir, "DESeq2_LFCshrink_V2_vs_A.csv")
lfc_V2_vs_V1_csv <- file.path(res_dir, "DESeq2_LFCshrink_V2_vs_V1.csv")

# Safety checks f√ºr coef-Namen
rn <- resultsNames(dds)
message("‚ÑπÔ∏è resultsNames(dds): ", paste(rn, collapse = ", "))

stopifnot("timepoint_V1_vs_A" %in% rn)
stopifnot("timepoint_V2_vs_A" %in% rn)

resLFC_V1_vs_A <- run_or_load_rds(
  rds_path = lfc_V1_vs_A_rds,
  csv_path = lfc_V1_vs_A_csv,
  compute_expr = quote(
    lfcShrink(
      dds,
      coef = "timepoint_V1_vs_A",
      res  = res_V1_vs_A,
      type = "apeglm"
    )
  )
)

resLFC_V2_vs_A <- run_or_load_rds(
  rds_path = lfc_V2_vs_A_rds,
  csv_path = lfc_V2_vs_A_csv,
  compute_expr = quote(
    lfcShrink(
      dds,
      coef = "timepoint_V2_vs_A",
      res  = res_V2_vs_A,
      type = "apeglm"
    )
  )
)

resLFC_V2_vs_V1 <- run_or_load_rds(
  rds_path = lfc_V2_vs_V1_rds,
  csv_path = lfc_V2_vs_V1_csv,
  compute_expr = quote(
    lfcShrink(
      dds,
      contrast = c("timepoint", "V2", "V1"),
      res      = res_V2_vs_V1,
      type     = "ashr"
    )
  )
)
```

```{r}
#==============================================================
# Dispersionskurve, um zu sehen, ob es Auff√§lligkeiten gibt

plotDispEsts(dds)
# Kein Hinweis auf Batch-Effekte, systematische Fehlanpassung oder kaputte Shrinkage
```

```{r}

# ============================================================
# Unshrunken vs. shrunken LFC vergleichen (MA-Plots)
# ============================================================

# -------------------------
# V1 vs A
# -------------------------
res_V1_vs_A_unshrunken <- res_V1_vs_A

DESeq2::plotMA(
  res_V1_vs_A_unshrunken,
  ylim = c(-2, 2),
  main = "V1 vs A ‚Äì unshrunken"
)

DESeq2::plotMA(
  resLFC_V1_vs_A,
  ylim = c(-2, 2),
  main = "V1 vs A ‚Äì shrunken"
)

# -------------------------
# V2 vs A
# -------------------------
res_V2_vs_A_unshrunken <- res_V2_vs_A

DESeq2::plotMA(
  res_V2_vs_A_unshrunken,
  ylim = c(-2, 2),
  main = "V2 vs A ‚Äì unshrunken"
)

DESeq2::plotMA(
  resLFC_V2_vs_A,
  ylim = c(-2, 2),
  main = "V2 vs A ‚Äì shrunken"
)

# -------------------------
# V2 vs V1
# -------------------------
res_V2_vs_V1_unshrunken <- res_V2_vs_V1

DESeq2::plotMA(
  res_V2_vs_V1_unshrunken,
  ylim = c(-2, 2),
  main = "V2 vs V1 ‚Äì unshrunken"
)

DESeq2::plotMA(
  resLFC_V2_vs_V1,
  ylim = c(-2, 2),
  main = "V2 vs V1 ‚Äì shrunken"
)

```

```{r}
# ============================================================
# 05c ‚Äì Summarizing results from the Wald test (HBC)
# Ziel:
#  1) Ergebnisse je Vergleich mit summary() zusammenfassen (alpha explizit)
#  2) padj Cutoff setzen
#  3) DESeqResults -> tibble umwandeln
#  4) signifikante Gene herausfiltern (padj < cutoff)
#  5) (praktisch) Anzahl up/down z√§hlen + Genlisten erzeugen
# ============================================================

# ------------------------------------------------------------
# Schritt 1 (HBC): Ergebnisse zusammenfassen mit summary()
# Wichtig: alpha explizit setzen, sonst ist Default padj < 0.1
# ------------------------------------------------------------
alpha_level <- 0.05

summary(res_V1_vs_A,  alpha = alpha_level)
summary(res_V2_vs_A,  alpha = alpha_level)
summary(res_V2_vs_V1, alpha = alpha_level)

# ------------------------------------------------------------
# Schritt 2 (HBC): Schwellenwerte definieren
# In 05c wird nur padj verwendet (keine LFC-Schwelle in dieser Lesson)
# ------------------------------------------------------------
padj_cutoff <- 0.05

# ------------------------------------------------------------
# Schritt 3 (HBC): DESeqResults -> tibble konvertieren
# (damit man bequem filtern/arrangieren kann)
# ------------------------------------------------------------
res_V1_vs_A_tb <- res_V1_vs_A |>
  data.frame() |>
  rownames_to_column(var = "gene") |>
  as_tibble()

res_V2_vs_A_tb <- res_V2_vs_A |>
  data.frame() |>
  rownames_to_column(var = "gene") |>
  as_tibble()

res_V2_vs_V1_tb <- res_V2_vs_V1 |>
  data.frame() |>
  rownames_to_column(var = "gene") |>
  as_tibble()

# ------------------------------------------------------------
# Schritt 4 (HBC): signifikante Gene extrahieren (padj < cutoff)
# Tipp: !is.na(padj) explizit, damit NAs nicht ‚Äûdurchrutschen‚Äú
# ------------------------------------------------------------
sig_V1_vs_A <- dplyr::filter(
  res_V1_vs_A_tb,
  !is.na(padj),
  padj < padj_cutoff
)

sig_V2_vs_A <- dplyr::filter(
  res_V2_vs_A_tb,
  !is.na(padj),
  padj < padj_cutoff
)

sig_V2_vs_V1 <- dplyr::filter(
  res_V2_vs_V1_tb,
  !is.na(padj),
  padj < padj_cutoff
)

# Optional: kurz anschauen
sig_V1_vs_A
sig_V2_vs_A
sig_V2_vs_V1

# ------------------------------------------------------------
# Schritt 5 (praktisch, passt zu HBC-Lernziel "Evaluate number of DE genes"):
# Up/Down z√§hlen und eine kleine √úbersicht bauen
# Up/Down bezieht sich auf das Vorzeichen von log2FoldChange
# ------------------------------------------------------------
count_up_down <- function(sig_tbl) {
  sig_tbl |>
    summarise(
      n_sig  = n(),
      n_up   = sum(log2FoldChange > 0, na.rm = TRUE),
      n_down = sum(log2FoldChange < 0, na.rm = TRUE)
    )
}

overview <- dplyr::bind_rows(
  count_up_down(sig_V1_vs_A)  |> dplyr::mutate(comparison = "V1 vs A"),
  count_up_down(sig_V2_vs_A)  |> dplyr::mutate(comparison = "V2 vs A"),
  count_up_down(sig_V2_vs_V1) |> dplyr::mutate(comparison = "V2 vs V1")
) |>
  dplyr::select(comparison, n_sig, n_up, n_down)

# ------------------------------------------------------------
# Schritt 6 (praktisch): Genlisten erzeugen (f√ºr Heatmaps/Enrichment etc.)
# ------------------------------------------------------------
genes_sig_V1_vs_A  <- sig_V1_vs_A$gene
genes_sig_V2_vs_A  <- sig_V2_vs_A$gene
genes_sig_V2_vs_V1 <- sig_V2_vs_V1$gene

# Optional: Top-Gene-Listen (z.B. nach padj sortiert)
top20_V1_vs_A <- sig_V1_vs_A |> arrange(padj) |> slice_head(n = 20)
top20_V2_vs_A <- sig_V2_vs_A |> arrange(padj) |> slice_head(n = 20)
top20_V2_vs_V1 <- sig_V2_vs_V1 |> arrange(padj) |> slice_head(n = 20)

# Optional: Export (falls gew√ºnscht)
# write.csv(sig_V1_vs_A,  "../../res/sig_V1_vs_A_padj05.csv", row.names = FALSE)
# write.csv(sig_V2_vs_A,  "../../res/sig_V2_vs_A_padj05.csv", row.names = FALSE)
# write.csv(sig_V2_vs_V1, "../../res/sig_V2_vs_V1_padj05.csv", row.names = FALSE)
# write.csv(overview,     "../../res/DE_summary_overview_padj05.csv", row.names = FALSE)

```
