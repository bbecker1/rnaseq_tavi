---
title: "DEanalysis"
---

<https://hbctraining.github.io/Intro-to-DGE/schedule/links-to-lessons.html>

```{r}
#Pakete werden aus Setup Skript geladen
source("../../org/setup_packages.R")

library(tidyverse)  
library(readxl)
library(janitor)
library(DESeq2)
library(tximport)
library(pheatmap)
library(RColorBrewer)
library(ggrepel)
library(cowplot)
library(DEGreport)
library(clusterProfiler)
library(DOSE)
library(org.Hs.eg.db)
library(pathview)
library(AnnotationHub)
library(ensembldb)
library(apeglm)
library(ashr)

sessionInfo()
```

```{r}
#| echo: false
#| warning: false
# Pfade
salmon <- "../../dat/salmon_files"
tx2g   <- "../../dat/info_files/tx2gene.tsv"
excel  <- "../../dat/info_files/CCGA_sequencing_NM_ventricular_dysfunction_RNAseq_2342.xlsx"

# Salmon-Files einlesen
files <- list.files(
  path      = salmon,
  full.names = TRUE,
  pattern   = "_quant\\.sf$"
)

# Saubere Namen f√ºr tximport
names(files) <- basename(files) %>%
  str_remove("_quant\\.sf$") %>%    # Endung entfernen
  str_remove("-L[0-9]+$")           # evtl. Library-Nummer entfernen

# Duplikate ausschlie√üen
stopifnot(length(files) > 1)
stopifnot(!anyDuplicated(names(files)))

# tx2gene einlesen
tx2gene <- read.delim(tx2g)
tx2gene[,1] <- sub("\\..*$", "", tx2gene[,1])  # Versionsnummer entfernen
tx2gene <- tx2gene[, c(1,2)]                    # Nur tx_id und gene_id

# tximport
txi_cache <- "../../res/cache/txi_salmon_tximport.rds"
dir.create(dirname(txi_cache), recursive = TRUE, showWarnings = FALSE)

if (file.exists(txi_cache)) {
  message("‚úÖ Lade tximport Cache: ", txi_cache)
  txi <- readRDS(txi_cache)
} else {
  message("‚è≥ Rechne tximport‚Ä¶")
  txi <- tximport(
    files,
    type = "salmon",
    tx2gene = tx2gene,
    countsFromAbundance = "lengthScaledTPM",
    ignoreTxVersion = TRUE
  )
  saveRDS(txi, txi_cache)
  message("üíæ Gespeichert: ", txi_cache)
}

# Sample-Namen in txi bereinigen
colnames(txi$counts)    <- str_remove(colnames(txi$counts), "_quant\\.sf$")
colnames(txi$abundance) <- str_remove(colnames(txi$abundance), "_quant\\.sf$")
colnames(txi$length)    <- str_remove(colnames(txi$length), "_quant\\.sf$")

# Metadata einlesen
md <- read_excel(excel, sheet = "Metadata", skip = 1) %>%
  janitor::clean_names() %>%
  mutate(sample_id = str_remove(library_id, "-L[0-9]+$") %>% str_trim())

input <- read_excel(excel, sheet = 3, skip = 1) %>%
  janitor::clean_names() %>%
  mutate(
    valve_treated = if_else(str_starts(sample_name, regex("^t", TRUE)),
                            "tricuspid valve",
                            "aortic valve"),
    timepoint = str_extract(sample_name, "(?<=_).*"),
    pid       = str_extract(sample_name, "^[^_]+")
  ) %>%
  dplyr::select(external_id, valve_treated, timepoint, pid)

md <- left_join(md, input, by = "external_id")
stopifnot(all(!is.na(md$valve_treated)))

# ============================================================
# Diagnostik: Counts-Samples vs. Metadata-Samples
# ============================================================
counts_samples <- colnames(txi$counts)

in_counts_not_meta <- setdiff(counts_samples, md$sample_id)
in_meta_not_counts <- setdiff(md$sample_id, counts_samples)

message("Samples in counts but not metadata: ", length(in_counts_not_meta))
message("Samples in metadata but not counts: ", length(in_meta_not_counts))

if (length(in_counts_not_meta) > 0) {
  message("‚Üí In counts, not in metadata:")
  print(in_counts_not_meta)
}

if (length(in_meta_not_counts) > 0) {
  message("‚Üí In metadata, not in counts:")
  print(in_meta_not_counts)
}

# Abbrechen, wenn es Mismatches gibt
stopifnot(length(in_counts_not_meta) == 0, length(in_meta_not_counts) == 0)


# Nur Aorten-Samples behalten
keep_aortic <- intersect(colnames(txi$counts),
                         md$sample_id[md$valve_treated == "aortic valve"])

stopifnot(length(keep_aortic) > 1)
stopifnot(!anyDuplicated(keep_aortic))

txi_aortic <- list(
  counts    = txi$counts[, keep_aortic, drop = FALSE],
  abundance = txi$abundance[, keep_aortic, drop = FALSE],
  length    = txi$length[, keep_aortic, drop = FALSE]
)


md_aortic <- md[md$sample_id %in% keep_aortic, ]

cat("Anzahl Aorten-Samples:", length(keep_aortic), "\n")
head(md_aortic)


```

```{r}
#| echo: false
#| warning: false

### Exploring RNA-seq count data

#Die Count-Verteilung einer einzelnen Probe ansehen
counts_aortic <- as.data.frame(txi_aortic$counts)

sample <- colnames(counts_aortic)[1]

ggplot(counts_aortic) +
  geom_histogram(aes(x = .data[[sample]]), bins = 200) +
  xlab("Raw expression counts") +
  ylab("Number of genes")

# Mean‚ÄìVariance Plot (RNA-seq count data) f√ºr Aorten-Counts

# 2) Mean und Varianz pro Gen (√ºber alle ausgew√§hlten Samples/Spalten)
mean_counts     <- apply(counts_aortic, 1, mean, na.rm = TRUE)  # '1' = zeilenweise (Gene)
variance_counts <- apply(counts_aortic, 1, var,  na.rm = TRUE)

# 3) Dataframe f√ºr ggplot
df_mv <- data.frame(
  mean_counts = mean_counts,
  variance_counts = variance_counts
)

# Gene mit mean/var <= 0 entfernen (log10 braucht > 0)
df_mv <- dplyr::filter(
  df_mv,
  mean_counts > 0,
  variance_counts > 0
)

# 4) Plot: log10-mean vs log10-variance, rote Linie = x=y
ggplot(df_mv) +
  geom_point(aes(x = mean_counts, y = variance_counts), alpha = 0.4) +
  scale_x_log10(limits = c(1, 1e9)) +
  scale_y_log10(limits = c(1, 1e9)) +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  labs(
    title = "Mean‚ÄìVariance Relationship (Aortic valve RNA-seq counts)",
    x = "Mean of counts per gene (log10)",
    y = "Variance of counts per gene (log10)"
  )
# Daten passen nicht zur Poission-Verteilung, weil die Varianz >> Mittelwert (Overdispersion). Besonders bei h√∂her exprimierten Genene ist die Varianz viel gr√∂√üer. Das rechtfertigt die Verwendung der Negativ-Binomial-Modelle von DESeq2

# ============================================================
# DESeq2: Normalisierung (Size-Factor / "median-of-ratios")
# Kontext: Salmon -> tximport -> DESeq2
# Ziel: Normalisierte Counts erzeugen (nur f√ºr Visualisierung/QC),
#       NICHT als Input f√ºr DE-Tests verwenden.
# ============================================================

# ------------------------------------------------------------
# VORAUSSETZUNGEN (existieren bereits im Workspace)
#   txi        : komplettes tximport-Objekt (aus tximport())
#   txi_aortic : gefiltertes tximport-Objekt f√ºr Aorten-Samples
#               (list mit counts/abundance/length)
#   md_aortic  : Metadata passend zu Aorten-Samples
#               (mind. sample_id, pid, timepoint)
# ------------------------------------------------------------

# ------------------------------------------------------------
# 1) Metadata f√ºr DESeq2 vorbereiten
#    - sample_id muss character sein (IDs)
#    - pid/timepoint als factor (Design-Variablen)
#    - rownames(meta) m√ºssen exakt die Sample-IDs sein
# ------------------------------------------------------------
meta <- md_aortic %>%
  mutate(
    sample_id = as.character(sample_id),
    pid       = factor(pid),        # Patient-ID
    timepoint = factor(timepoint)   
  ) %>%
  as.data.frame()

rownames(meta) <- meta$sample_id

# ------------------------------------------------------------
# 2) Sicherstellen, dass Counts-Spalten und Metadata-Zeilen matchen
#    DESeq2 verlangt:
#      colnames(counts) == rownames(colData)
#    Reihenfolge ist wichtig (sonst falsche Zuordnung)
# ------------------------------------------------------------

# Check: alle Count-Sample-IDs existieren in meta?
stopifnot(all(colnames(txi_aortic$counts) %in% rownames(meta)))

# Meta exakt in die Reihenfolge der Count-Spalten bringen
meta <- meta[match(colnames(txi_aortic$counts), rownames(meta)), , drop = FALSE]

# Check: jetzt exakt identisch und in gleicher Reihenfolge?
stopifnot(all(colnames(txi_aortic$counts) == rownames(meta)))

# ------------------------------------------------------------
# 3) tximport-Metainfo erhalten
#    DESeqDataSetFromTximport() erwartet u.a. txi$countsFromAbundance
#    Wenn txi_aortic manuell als Liste gebaut wurde, fehlt das
# ------------------------------------------------------------
txi_aortic$countsFromAbundance <- txi$countsFromAbundance

# ------------------------------------------------------------
# 4) DESeqDataSet erstellen
#    Design: ~ pid + timepoint
#    - pid modelliert patientenspezifische Baselines
#    - timepoint testet die systematische √Ñnderung √ºber die Zeit
# ------------------------------------------------------------
dds <- DESeqDataSetFromTximport(
  txi = txi_aortic,
  colData = meta,
  design = ~ pid + timepoint
)

# ------------------------------------------------------------
# 5) Prefiltering:
#    Entfernt Gene mit extrem niedrigen Counts in allen Samples,
#    verbessert Laufzeit und reduziert Noise.
# ------------------------------------------------------------
dds <- dds[rowSums(counts(dds)) >= 10, ]

# ------------------------------------------------------------
# 6) Normalisierung: Size Factors sch√§tzen
#    DESeq2 nutzt "median-of-ratios" (robust gegen Outlier/Geneffekte)
# ------------------------------------------------------------
dds <- estimateSizeFactors(dds)

# Size factors inspizieren (sollten grob um 1 liegen; Unterschiede = Library size etc.)
sf <- sizeFactors(dds)
print(sf)

# ------------------------------------------------------------
# 7) Normalisierte Counts extrahieren
#    Diese Matrix ist sinnvoll f√ºr:
#      - PCA / Heatmaps / Plots
#    NICHT sinnvoll als Input f√ºr DESeq2-DE-Tests (die arbeiten mit Rohcounts + Modell)
# ------------------------------------------------------------
normalized_counts <- counts(dds, normalized = TRUE)

# ------------------------------------------------------------
# 8) Speichern
# ------------------------------------------------------------
write.table(
  normalized_counts,
  file = "../../res/normalized_counts_aortic_DESeq2.tsv",
  sep = "\t",
  quote = FALSE,
  col.names = NA
)
```

```{r}
### ============================================================
### Quality Control (QC) ‚Äì DESeq2 RNA-seq
### ============================================================

# ------------------------------------------------------------
# 1) Varianz-stabilisierende Transformation
# ------------------------------------------------------------
trans <- vst(dds, blind = TRUE)

# ------------------------------------------------------------
# 2) PCA ‚Äì schnelle √úbersicht
# ------------------------------------------------------------
plotPCA(trans, intgroup = "timepoint")

# ------------------------------------------------------------
# 3) PCA ‚Äì angepasstes ggplot (PC1 vs PC2)
# ------------------------------------------------------------
pca_df <- plotPCA(trans, intgroup = "timepoint", returnData = TRUE)
percentVar <- round(100 * attr(pca_df, "percentVar"))

ggplot(pca_df, aes(x = PC1, y = PC2, color = timepoint)) +
  geom_point(size = 3) +
  xlab(paste0("PC1: ", percentVar[1], "% Varianz")) +
  ylab(paste0("PC2: ", percentVar[2], "% Varianz")) +
  theme_minimal()

# ------------------------------------------------------------
# 4) PCA ‚Äì h√∂here Komponenten (PC3 vs PC4) mit vst-Matrix
# ------------------------------------------------------------
trans_mat <- assay(trans)        # Gene x Samples
pca <- prcomp(t(trans_mat))      # PCA auf Samples

df_pc <- cbind(
  as.data.frame(colData(dds)),
  as.data.frame(pca$x)
)

ggplot(df_pc, aes(x = PC3, y = PC4, color = timepoint)) +
  geom_point(size = 3) +
  theme_minimal()

##Die PCAs clustern eher nicht nach Timepoint, weil der Patienteneffekt > Timepoint-Effekt. Die Expression eines Patienten √ºber mehrere Zeitpunkte ist √§hnlicher als die Expression verschiedener Patienten √ºber einem Zeitpunkt.
# ------------------------------------------------------------
# 5) Hierarchisches Clustering ‚Äì Sample-Sample-Korrelation + Heatmap
# ------------------------------------------------------------
sample_cor <- cor(trans_mat)     # Samples x Samples

anno <- as.data.frame(colData(dds)[, "timepoint", drop = FALSE])
anno$timepoint <- factor(anno$timepoint)

anno <- anno[match(colnames(sample_cor), rownames(anno)), , drop = FALSE]
stopifnot(all(rownames(anno) == colnames(sample_cor)))

pheatmap(sample_cor,
         annotation_col = anno,
         show_colnames = FALSE,
         show_rownames = FALSE,
         breaks = seq(0.9, 1, length.out = 100) #damit werden feine Unterschiede                                                    etwas sichtbarer
         )
##Heatmap zeigt hohe Korrelationen, was f√ºr technisch saubere Daten spricht

```

```{r}
# ============================================================
# DESeq2 DE-Analyse nur wenn n√∂tig (Caching via RDS-Dateien)
# - Rechnet DESeq(dds) nur, wenn fitted dds noch nicht gespeichert ist
# - Berechnet/ speichert Result- und Shrinkage-Tabellen nur, wenn nicht vorhanden
# Voraussetzung: Objekt 'dds' (unfitted) existiert bereits in der Session
# ============================================================

# Speicherorte
cache_dir <- "../../res/cache"
if (!dir.exists(cache_dir)) dir.create(cache_dir, recursive = TRUE)

dds_cache_file <- file.path(cache_dir, "dds_DESeq_fitted.rds")

# 1) Fitted dds laden oder berechnen
if (file.exists(dds_cache_file)) {

  message("Lade bereits berechnetes DESeq2-Objekt: ", dds_cache_file)
  dds <- readRDS(dds_cache_file)

} else {

  message("Berechne DESeq2 (kann dauern)‚Ä¶")

  # sicherstellen, dass timepoint korrekt als Faktor vorliegt
  colData(dds)$timepoint <- factor(colData(dds)$timepoint, levels = c("A", "V1", "V2"))

  # Design setzen
  design(dds) <- ~ pid + timepoint

  # Prefiltering
  dds <- dds[rowSums(counts(dds)) >= 10, ]

  # DESeq laufen lassen
  dds <- DESeq(dds)

  # speichern
  saveRDS(dds, dds_cache_file)
  message("Gespeichert: ", dds_cache_file)
}

# ------------------------------------------------------------
# Helper: RDS-first caching (optional zus√§tzlich CSV-Export)
# ------------------------------------------------------------
run_or_load_rds <- function(rds_path, compute_expr, csv_path = NULL) {
  if (file.exists(rds_path)) {
    message("Lade: ", rds_path)
    return(readRDS(rds_path))
  }

  message("Berechne & speichere: ", rds_path)
  out <- eval(compute_expr)

  saveRDS(out, rds_path)

  if (!is.null(csv_path)) {
    # F√ºr Sharing/Excel etc. (Attributes gehen in CSV verloren ‚Äì ist okay)
    write.csv(as.data.frame(out), csv_path)
    message("Zus√§tzlich als CSV exportiert: ", csv_path)
  }

  out
}

# ------------------------------------------------------------
# 2) Raw Results (Wald-Tests) + RDS/CSV Cache
# ------------------------------------------------------------
alpha_level <- 0.05

res_dir <- "../../res"
if (!dir.exists(res_dir)) dir.create(res_dir, recursive = TRUE)

res_V1_vs_A_rds  <- file.path(cache_dir, "DESeq2_results_V1_vs_A.rds")
res_V2_vs_A_rds  <- file.path(cache_dir, "DESeq2_results_V2_vs_A.rds")
res_V2_vs_V1_rds <- file.path(cache_dir, "DESeq2_results_V2_vs_V1.rds")

res_V1_vs_A_csv  <- file.path(res_dir, "DESeq2_results_V1_vs_A.csv")
res_V2_vs_A_csv  <- file.path(res_dir, "DESeq2_results_V2_vs_A.csv")
res_V2_vs_V1_csv <- file.path(res_dir, "DESeq2_results_V2_vs_V1.csv")

res_V1_vs_A <- run_or_load_rds(
  rds_path = res_V1_vs_A_rds,
  csv_path = res_V1_vs_A_csv,
  compute_expr = quote(
    results(dds, contrast = c("timepoint", "V1", "A"), alpha = alpha_level)
  )
)

res_V2_vs_A <- run_or_load_rds(
  rds_path = res_V2_vs_A_rds,
  csv_path = res_V2_vs_A_csv,
  compute_expr = quote(
    results(dds, contrast = c("timepoint", "V2", "A"), alpha = alpha_level)
  )
)

res_V2_vs_V1 <- run_or_load_rds(
  rds_path = res_V2_vs_V1_rds,
  csv_path = res_V2_vs_V1_csv,
  compute_expr = quote(
    results(dds, contrast = c("timepoint", "V2", "V1"), alpha = alpha_level)
  )
)

# ------------------------------------------------------------
# 3) Shrinkage + RDS/CSV Cache
#    - apeglm via coef (f√ºr timepoint-Koeffizienten)
#    - ashr via contrast (geht gut f√ºr beliebige Kontraste)
#    - UND: res=... koppeln (Konsistenz)
# ------------------------------------------------------------
lfc_V1_vs_A_rds  <- file.path(cache_dir, "DESeq2_LFCshrink_V1_vs_A.rds")
lfc_V2_vs_A_rds  <- file.path(cache_dir, "DESeq2_LFCshrink_V2_vs_A.rds")
lfc_V2_vs_V1_rds <- file.path(cache_dir, "DESeq2_LFCshrink_V2_vs_V1.rds")

lfc_V1_vs_A_csv  <- file.path(res_dir, "DESeq2_LFCshrink_V1_vs_A.csv")
lfc_V2_vs_A_csv  <- file.path(res_dir, "DESeq2_LFCshrink_V2_vs_A.csv")
lfc_V2_vs_V1_csv <- file.path(res_dir, "DESeq2_LFCshrink_V2_vs_V1.csv")

# Safety checks f√ºr coef-Namen
rn <- resultsNames(dds)
message("resultsNames(dds): ", paste(rn, collapse = ", "))

stopifnot("timepoint_V1_vs_A" %in% rn)
stopifnot("timepoint_V2_vs_A" %in% rn)

resLFC_V1_vs_A <- run_or_load_rds(
  rds_path = lfc_V1_vs_A_rds,
  csv_path = lfc_V1_vs_A_csv,
  compute_expr = quote(
    lfcShrink(
      dds,
      coef = "timepoint_V1_vs_A",
      res  = res_V1_vs_A,
      type = "apeglm"
    )
  )
)

resLFC_V2_vs_A <- run_or_load_rds(
  rds_path = lfc_V2_vs_A_rds,
  csv_path = lfc_V2_vs_A_csv,
  compute_expr = quote(
    lfcShrink(
      dds,
      coef = "timepoint_V2_vs_A",
      res  = res_V2_vs_A,
      type = "apeglm"
    )
  )
)

resLFC_V2_vs_V1 <- run_or_load_rds(
  rds_path = lfc_V2_vs_V1_rds,
  csv_path = lfc_V2_vs_V1_csv,
  compute_expr = quote(
    lfcShrink(
      dds,
      contrast = c("timepoint", "V2", "V1"),
      res      = res_V2_vs_V1,
      type     = "ashr"
    )
  )
)
```

```{r}
#==============================================================
# Dispersionskurve, um zu sehen, ob es Auff√§lligkeiten gibt

plotDispEsts(dds)
# Kein Hinweis auf Batch-Effekte, systematische Fehlanpassung oder kaputte Shrinkage
```

```{r}

# ============================================================
# Unshrunken vs. shrunken LFC vergleichen (MA-Plots)
# ============================================================

# -------------------------
# V1 vs A
# -------------------------
res_V1_vs_A_unshrunken <- res_V1_vs_A

DESeq2::plotMA(
  res_V1_vs_A_unshrunken,
  ylim = c(-2, 2),
  main = "V1 vs A ‚Äì unshrunken"
)

DESeq2::plotMA(
  resLFC_V1_vs_A,
  ylim = c(-2, 2),
  main = "V1 vs A ‚Äì shrunken"
)

# -------------------------
# V2 vs A
# -------------------------
res_V2_vs_A_unshrunken <- res_V2_vs_A

DESeq2::plotMA(
  res_V2_vs_A_unshrunken,
  ylim = c(-2, 2),
  main = "V2 vs A ‚Äì unshrunken"
)

DESeq2::plotMA(
  resLFC_V2_vs_A,
  ylim = c(-2, 2),
  main = "V2 vs A ‚Äì shrunken"
)

# -------------------------
# V2 vs V1
# -------------------------
res_V2_vs_V1_unshrunken <- res_V2_vs_V1

DESeq2::plotMA(
  res_V2_vs_V1_unshrunken,
  ylim = c(-2, 2),
  main = "V2 vs V1 ‚Äì unshrunken"
)

DESeq2::plotMA(
  resLFC_V2_vs_V1,
  ylim = c(-2, 2),
  main = "V2 vs V1 ‚Äì shrunken"
)

```

```{r}
# ============================================================
# Summarizing results from the Wald test
# Ziel:
#  1) Ergebnisse je Vergleich mit summary() zusammenfassen (alpha explizit)
#  2) padj Cutoff setzen
#  3) DESeqResults -> tibble umwandeln
#  4) signifikante Gene herausfiltern (padj < cutoff)
#  5) (praktisch) Anzahl up/down z√§hlen + Genlisten erzeugen
# ============================================================

# ------------------------------------------------------------
# Schritt 1: Ergebnisse zusammenfassen mit summary()
# alpha explizit setzen, sonst ist Default padj < 0.1
# ------------------------------------------------------------
alpha_level <- 0.05

summary(res_V1_vs_A,  alpha = alpha_level)
summary(res_V2_vs_A,  alpha = alpha_level)
summary(res_V2_vs_V1, alpha = alpha_level)

# ------------------------------------------------------------
# Schritt 2: Schwellenwerte definieren
# ------------------------------------------------------------
padj_cutoff <- 0.05

# ------------------------------------------------------------
# Schritt 3: DESeqResults -> tibble konvertieren
# (damit man bequem filtern/arrangieren kann)
# ------------------------------------------------------------
res_V1_vs_A_tb <- res_V1_vs_A |>
  data.frame() |>
  rownames_to_column(var = "gene") |>
  as_tibble()

res_V2_vs_A_tb <- res_V2_vs_A |>
  data.frame() |>
  rownames_to_column(var = "gene") |>
  as_tibble()

res_V2_vs_V1_tb <- res_V2_vs_V1 |>
  data.frame() |>
  rownames_to_column(var = "gene") |>
  as_tibble()

# ------------------------------------------------------------
# Schritt 4: signifikante Gene extrahieren (padj < cutoff)
# !is.na(padj) explizit, damit NAs nicht ‚Äûdurchrutschen‚Äú
# ------------------------------------------------------------
sig_V1_vs_A <- dplyr::filter(
  res_V1_vs_A_tb,
  !is.na(padj),
  padj < padj_cutoff
)

sig_V2_vs_A <- dplyr::filter(
  res_V2_vs_A_tb,
  !is.na(padj),
  padj < padj_cutoff
)

sig_V2_vs_V1 <- dplyr::filter(
  res_V2_vs_V1_tb,
  !is.na(padj),
  padj < padj_cutoff
)

# Optional: kurz anschauen
sig_V1_vs_A
sig_V2_vs_A
sig_V2_vs_V1

# ------------------------------------------------------------
# Schritt 5:
# Up/Down z√§hlen und eine kleine √úbersicht bauen
# Up/Down bezieht sich auf das Vorzeichen von log2FoldChange
# ------------------------------------------------------------
count_up_down <- function(sig_tbl) {
  sig_tbl |>
    summarise(
      n_sig  = n(),
      n_up   = sum(log2FoldChange > 0, na.rm = TRUE),
      n_down = sum(log2FoldChange < 0, na.rm = TRUE)
    )
}

overview <- dplyr::bind_rows(
  count_up_down(sig_V1_vs_A)  |> dplyr::mutate(comparison = "V1 vs A"),
  count_up_down(sig_V2_vs_A)  |> dplyr::mutate(comparison = "V2 vs A"),
  count_up_down(sig_V2_vs_V1) |> dplyr::mutate(comparison = "V2 vs V1")
) |>
  dplyr::select(comparison, n_sig, n_up, n_down)

# ------------------------------------------------------------
# Schritt 6: Genlisten erzeugen (f√ºr Heatmaps/Enrichment etc.)
# ------------------------------------------------------------
genes_sig_V1_vs_A  <- sig_V1_vs_A$gene
genes_sig_V2_vs_A  <- sig_V2_vs_A$gene
genes_sig_V2_vs_V1 <- sig_V2_vs_V1$gene

#Top-Gene-Listen (z.B. nach padj sortiert)
top20_V1_vs_A <- sig_V1_vs_A |> arrange(padj) |> slice_head(n = 20)
top20_V2_vs_A <- sig_V2_vs_A |> arrange(padj) |> slice_head(n = 20)
top20_V2_vs_V1 <- sig_V2_vs_V1 |> arrange(padj) |> slice_head(n = 20)

# ggf: Export
# write.csv(sig_V1_vs_A,  "../../res/sig_V1_vs_A_padj05.csv", row.names = FALSE)
# write.csv(sig_V2_vs_A,  "../../res/sig_V2_vs_A_padj05.csv", row.names = FALSE)
# write.csv(sig_V2_vs_V1, "../../res/sig_V2_vs_V1_padj05.csv", row.names = FALSE)
# write.csv(overview,     "../../res/DE_summary_overview_padj05.csv", row.names = FALSE)

```

\# --- Meta vorbereiten ---

sample_meta \<- meta \|\>

rownames_to_column(var = "samplename") \|\>

as_tibble()

stopifnot(all(sample_meta\$samplename %in% colnames(normalized_counts)))

sample_meta \<- sample_meta \|\>

mutate(condition = droplevels(timepoint))

\# --- Normalisierte Counts als Tibble ---

norm_counts_tb \<- normalized_counts \|\>

as.data.frame() \|\>

rownames_to_column(var = "gene") \|\>

as_tibble()

\# --- Metadaten pro Kontrast filtern ---

meta_v1_a \<- sample_meta \|\> dplyr::filter(condition %in% c("A", "V1"))

meta_v2_a \<- sample_meta \|\> dplyr::filter(condition %in% c("A", "V2"))

meta_v2_v1 \<- sample_meta \|\> dplyr::filter(condition %in% c("V1", "V2"))

\# --- Plot-Funktion (mit +1 Pseudocount vor log10) ---

plot_top20_expression \<- function(res_tb, norm_counts_tb, meta_tb,

padj_cutoff = 0.05,

n_top = 20,

title = NULL) {

top_genes \<- res_tb \|\>

dplyr::filter(!is.na(padj), padj \< padj_cutoff) \|\>

dplyr::arrange(padj) \|\>

dplyr::slice_head(n = n_top) \|\>

dplyr::pull(gene)

plot_df \<- norm_counts_tb \|\>

dplyr::filter(gene %in% top_genes) \|\>

tidyr::pivot_longer(

cols = -gene,

names_to = "samplename",

values_to = "norm_count"

) \|\>

dplyr::inner_join(meta_tb, by = "samplename")

ggplot(plot_df, aes(x = gene, y = norm_count + 1, color = condition)) +

geom_point(position = position_jitter(width = 0.15), alpha = 0.8) +

scale_y_log10() +

labs(

title = title,

x = NULL,

y = "Normalized counts (log10, +1)",

color = "timepoint"

) +

theme_bw() +

theme(

axis.text.x = element_text(angle = 45, hjust = 1),

panel.grid.minor = element_blank()

)

}

\# --- Plots erzeugen ---

p_v1_a \<- plot_top20_expression(

res_tb = res_V1_vs_A_tb,

norm_counts_tb = norm_counts_tb,

meta_tb = meta_v1_a,

title = "Top 20 DE genes: V1 vs A"

)

p_v2_a \<- plot_top20_expression(

res_tb = res_V2_vs_A_tb,

norm_counts_tb = norm_counts_tb,

meta_tb = meta_v2_a,

title = "Top 20 DE genes: V2 vs A"

)

p_v2_v1 \<- plot_top20_expression(

res_tb = res_V2_vs_V1_tb,

norm_counts_tb = norm_counts_tb,

meta_tb = meta_v2_v1,

title = "Top 20 DE genes: V2 vs V1"

)

p_v1_a

p_v2_a

p_v2_v1

```{r}
# ============================================================
# DESeq2-Visualisierungen + Genannotation
#
# Inhalte:
#  1) Vorbereitung: Metadaten + normalisierte Counts
#  2) Genannotation (Ensembl -> SYMBOL/GENENAME), Ensembl-Versionen entfernen
#  3) Top-20-Expressionplots (Counts, nicht shrunken)
#  4) Volcano Plots (padj aus unshrunken results)
#  5) Heatmaps signifikanter Gene (pheatmap, Counts)
# ============================================================
# -----------------------------
# 1) Vorbereitung: Metadaten + Checks
# -----------------------------

# Metadaten in ein Tibble √ºberf√ºhren (Rownames = Sample-Namen sichern)
sample_meta <- meta |>
  tibble::rownames_to_column(var = "samplename") |>
  tibble::as_tibble() |>
  dplyr::mutate(
    # Zeitpunkte A/V1/V2 als Plot-/Gruppierungsfaktor nutzen
    condition = droplevels(timepoint)
  )

# Sicherstellen, dass Sample-Namen in meta auch in den Count-Spalten existieren
stopifnot(all(sample_meta$samplename %in% colnames(normalized_counts)))

# Normalisierte Counts als Tibble (praktisch f√ºr ggplot-Pipelines)
norm_counts_tb <- normalized_counts |>
  as.data.frame() |>
  tibble::rownames_to_column(var = "gene") |>
  tibble::as_tibble()

# -----------------------------
# 2) Genannotation: Ensembl-Version entfernen + SYMBOL/GENENAME erg√§nzen
# -----------------------------

# Ensembl-ID ohne Versionssuffix (ENSG... .16 -> ENSG...)
strip_ensembl_version <- function(x) sub("\\..*$", "", x)

# Mapping-Tabelle einmalig erzeugen (schneller als 3x select)
# Wir sammeln alle Ensembl-IDs aus allen Result-Tabellen.
all_ensembl <- unique(c(
  strip_ensembl_version(res_V1_vs_A_tb$gene),
  strip_ensembl_version(res_V2_vs_A_tb$gene),
  strip_ensembl_version(res_V2_vs_V1_tb$gene)
))

# Annotation ziehen: ENSEMBL -> SYMBOL + GENENAME
gene_anno <- AnnotationDbi::select(
  org.Hs.eg.db,
  keys = all_ensembl,
  keytype = "ENSEMBL",
  columns = c("SYMBOL", "GENENAME")
) |>
  dplyr::as_tibble() |>
  dplyr::rename(ensembl_id = ENSEMBL) |>
  # Doppelte Eintr√§ge k√∂nnen vorkommen (z.B. veraltete/mehrdeutige Mappings)
  # Wir behalten pro ensembl_id den ersten nicht-NA SYMBOL, sonst irgendeinen.
  dplyr::arrange(ensembl_id, dplyr::desc(!is.na(SYMBOL))) |>
  dplyr::distinct(ensembl_id, .keep_all = TRUE)

# Helper: Annotation an eine Results-Tabelle h√§ngen (gene bleibt unver√§ndert!)
add_gene_symbols <- function(res_tb, gene_anno) {
  res_tb |>
    dplyr::mutate(ensembl_id = strip_ensembl_version(gene)) |>
    dplyr::left_join(gene_anno, by = "ensembl_id")
}

# Annotierte Results (f√ºr Tabellen/Interpretation/Labels)
res_V1_vs_A_annot  <- add_gene_symbols(res_V1_vs_A_tb, gene_anno)
res_V2_vs_A_annot  <- add_gene_symbols(res_V2_vs_A_tb, gene_anno)
res_V2_vs_V1_annot <- add_gene_symbols(res_V2_vs_V1_tb, gene_anno)

# Optional: auch signifikante Tabellen annotieren (praktisch f√ºr Exporte)
sig_V1_vs_A_annot  <- add_gene_symbols(sig_V1_vs_A, gene_anno)
sig_V2_vs_A_annot  <- add_gene_symbols(sig_V2_vs_A, gene_anno)
sig_V2_vs_V1_annot <- add_gene_symbols(sig_V2_vs_V1, gene_anno)

# Kurzer Sanity-Check
head(res_V1_vs_A_annot |> dplyr::select(gene, ensembl_id, SYMBOL, GENENAME), 10)
mean(is.na(res_V1_vs_A_annot$SYMBOL))

# -----------------------------
# 3) Kontrast-Definitionen zentral
# -----------------------------
contrasts <- list(
  v1_a = list(
    label = "V1 vs A",
    keep_levels = c("A", "V1"),
    res_tb = res_V1_vs_A_tb,
    res_annot = res_V1_vs_A_annot,
    sig_tb = sig_V1_vs_A,
    sig_annot = sig_V1_vs_A_annot
  ),
  v2_a = list(
    label = "V2 vs A",
    keep_levels = c("A", "V2"),
    res_tb = res_V2_vs_A_tb,
    res_annot = res_V2_vs_A_annot,
    sig_tb = sig_V2_vs_A,
    sig_annot = sig_V2_vs_A_annot
  ),
  v2_v1 = list(
    label = "V2 vs V1",
    keep_levels = c("V1", "V2"),
    res_tb = res_V2_vs_V1_tb,
    res_annot = res_V2_vs_V1_annot,
    sig_tb = sig_V2_vs_V1,
    sig_annot = sig_V2_vs_V1_annot
  )
)

# Helper: Metadaten f√ºr einen Kontrast (nur die relevanten Gruppen)
get_meta_for_contrast <- function(sample_meta, keep_levels) {
  sample_meta |>
    dplyr::filter(condition %in% keep_levels) |>
    droplevels()
}

# -----------------------------
# 4) Plot-Funktion: Top-20 Expression (Counts), mit +1 Pseudocount
# -----------------------------
plot_top20_expression <- function(res_tb, norm_counts_tb, meta_tb,
                                  padj_cutoff = 0.05,
                                  n_top = 20,
                                  title = NULL) {
  # Top-Gene nach padj (nur signifikante)
  top_genes <- res_tb |>
    dplyr::filter(!is.na(padj), padj < padj_cutoff) |>
    dplyr::arrange(padj) |>
    dplyr::slice_head(n = n_top) |>
    dplyr::pull(gene)

  plot_df <- norm_counts_tb |>
    dplyr::filter(gene %in% top_genes) |>
    tidyr::pivot_longer(
      cols = -gene,
      names_to = "samplename",
      values_to = "norm_count"
    ) |>
    dplyr::inner_join(meta_tb, by = "samplename")

  ggplot2::ggplot(plot_df, ggplot2::aes(x = gene, y = norm_count + 1, color = condition)) +
    ggplot2::geom_point(position = ggplot2::position_jitter(width = 0.15), alpha = 0.8) +
    ggplot2::scale_y_log10() +
    ggplot2::labs(
      title = title,
      x = NULL,
      y = "Normalisierte Counts (log10, +1)",
      color = "Zeitpunkt"
    ) +
    ggplot2::theme_bw() +
    ggplot2::theme(
      axis.text.x = ggplot2::element_text(angle = 45, hjust = 1),
      panel.grid.minor = ggplot2::element_blank()
    )
}

# -----------------------------
# 5) Plot-Funktion: Volcano Plot
# -----------------------------
plot_volcano <- function(res_tb, padj_cutoff = 0.05, title = NULL) {
  plot_df <- res_tb |>
    dplyr::filter(!is.na(padj)) |>
    dplyr::mutate(
      neg_log10_padj = -log10(padj),
      sig = padj < padj_cutoff
    )

  ggplot2::ggplot(plot_df, ggplot2::aes(x = log2FoldChange, y = neg_log10_padj)) +
    ggplot2::geom_point(ggplot2::aes(color = sig), alpha = 0.6, size = 1.2) +
    ggplot2::scale_color_manual(values = c("FALSE" = "grey70", "TRUE" = "red")) +
    ggplot2::geom_hline(
      yintercept = -log10(padj_cutoff),
      linetype = "dashed"
    ) +
    ggplot2::labs(
      title = title,
      x = "log2 Fold Change",
      y = "-log10 adjustierter p-Wert",
      color = paste0("padj < ", padj_cutoff)
    ) +
    ggplot2::theme_bw()
}

# -----------------------------
# 6) Plot-Funktion: Heatmap signifikanter Gene (Counts, pheatmap)
# -----------------------------
plot_sig_heatmap <- function(normalized_counts, sample_meta, sig_genes, keep_levels,
                             heat_colors = RColorBrewer::brewer.pal(6, "YlOrRd"),
                             show_rownames = FALSE,
                             scale = "row",
                             main = NULL) {
  # Samples f√ºr diesen Kontrast
  samples_keep <- sample_meta |>
    dplyr::filter(condition %in% keep_levels) |>
    dplyr::pull(samplename)

  # Matrixsubset: nur signifikante Gene & relevante Samples
  mat <- normalized_counts[
    rownames(normalized_counts) %in% sig_genes,
    colnames(normalized_counts) %in% samples_keep,
    drop = FALSE
  ]

  if (nrow(mat) == 0) {
    stop("Keine signifikanten Gene f√ºr diesen Kontrast (nrow(mat) == 0).")
  }

  # Annotation f√ºr Spalten (Samples sind Spalten)
  annotation_col <- sample_meta |>
    dplyr::filter(samplename %in% colnames(mat)) |>
    dplyr::select(condition) |>
    as.data.frame()

  rownames(annotation_col) <- sample_meta |>
    dplyr::filter(samplename %in% colnames(mat)) |>
    dplyr::pull(samplename)

  pheatmap::pheatmap(
    mat,
    color = heat_colors,
    cluster_rows = TRUE,
    cluster_cols = TRUE,
    show_rownames = show_rownames,
    annotation_col = annotation_col,
    border_color = NA,
    fontsize = 10,
    scale = scale,
    main = main
  )
}

# ============================================================
# 7) Ausf√ºhren: Top-20, Volcano, Heatmaps f√ºr alle Kontraste
# ============================================================

padj_cutoff <- 0.05
heat_colors <- RColorBrewer::brewer.pal(6, "YlOrRd")

# --- Top-20 Expressionplots ---
top20_plots <- lapply(contrasts, function(x) {
  meta_sub <- get_meta_for_contrast(sample_meta, x$keep_levels)
  plot_top20_expression(
    res_tb = x$res_tb,
    norm_counts_tb = norm_counts_tb,
    meta_tb = meta_sub,
    padj_cutoff = padj_cutoff,
    n_top = 20,
    title = paste0("Top 20 DE-Gene: ", x$label)
  )
})

top20_plots$v1_a
top20_plots$v2_a
top20_plots$v2_v1

# --- Volcano Plots ---
volcano_plots <- lapply(contrasts, function(x) {
  plot_volcano(
    res_tb = x$res_tb,
    padj_cutoff = padj_cutoff,
    title = paste0("Volcano Plot: ", x$label)
  )
})

volcano_plots$v1_a
volcano_plots$v2_a
volcano_plots$v2_v1

# --- Heatmaps signifikanter Gene ---
# Hinweis: pheatmap zeichnet direkt; die Liste ist nur der Vollst√§ndigkeit halber.
heatmaps <- lapply(contrasts, function(x) {
  plot_sig_heatmap(
    normalized_counts = normalized_counts,
    sample_meta = sample_meta,
    sig_genes = x$sig_tb$gene,
    keep_levels = x$keep_levels,
    heat_colors = heat_colors,
    show_rownames = FALSE,
    scale = "row",
    main = paste0("Heatmap signifikanter Gene: ", x$label)
  )
})

# ============================================================
# 8) Top-Tabellen mit Gene Symbols (f√ºr Export/Interpretation)
# ============================================================

# Top 20 signifikante Gene mit SYMBOL f√ºr V1 vs A
top20_sig_v1_a <- sig_V1_vs_A_annot |>
  dplyr::filter(!is.na(padj)) |>
  dplyr::arrange(padj) |>
  dplyr::slice_head(n = 20) |>
  dplyr::select(gene, ensembl_id, SYMBOL, GENENAME, log2FoldChange, padj)

top20_sig_v1_a

# Top 20 signifikante Gene mit SYMBOL f√ºr V2 vs A
top20_sig_v2_a <- sig_V2_vs_A_annot |>
  dplyr::filter(!is.na(padj)) |>
  dplyr::arrange(padj) |>
  dplyr::slice_head(n = 20) |>
  dplyr::select(gene, ensembl_id, SYMBOL, GENENAME, log2FoldChange, padj)

top20_sig_v2_a


# Top 20 signifikante Gene mit SYMBOL f√ºr V2 vs V1
top20_sig_v2_v1 <- sig_V2_vs_V1_annot |>
  dplyr::filter(!is.na(padj)) |>
  dplyr::arrange(padj) |>
  dplyr::slice_head(n = 20) |>
  dplyr::select(gene, ensembl_id, SYMBOL, GENENAME, log2FoldChange, padj)

top20_sig_v2_v1



```

```{r}
# ============================================================
# LRT-Analyse "nur wenn n√∂tig" (Caching via RDS)
# Full:    ~ pid + timepoint
# Reduced: ~ pid
# ============================================================

library(DESeq2)

cache_dir <- "../../res/cache"
if (!dir.exists(cache_dir)) dir.create(cache_dir, recursive = TRUE)

dds_lrt_cache_file <- file.path(cache_dir, "dds_DESeq_LRT_fitted.rds")
res_lrt_rds        <- file.path(cache_dir, "DESeq2_results_LRT_timepoint.rds")
res_lrt_csv        <- file.path("../../res", "DESeq2_results_LRT_timepoint.csv")

# 1) LRT-fitted dds laden oder berechnen
if (file.exists(dds_lrt_cache_file)) {
  message("Lade bereits berechnetes LRT-DESeq2-Objekt: ", dds_lrt_cache_file)
  dds_lrt <- readRDS(dds_lrt_cache_file)
} else {
  message("Berechne DESeq2 LRT‚Ä¶")

  # Hier NICHT dein bereits gefittetes dds √ºberschreiben,
  # sondern mit einem frischen Objekt arbeiten
  dds_lrt <- dds

  colData(dds_lrt)$timepoint <- factor(colData(dds_lrt)$timepoint, levels = c("A", "V1", "V2"))
  design(dds_lrt) <- ~ pid + timepoint

  # gleiches Prefiltering wie bei Wald (optional, aber konsistent)
  dds_lrt <- dds_lrt[rowSums(counts(dds_lrt)) >= 10, ]

  dds_lrt <- DESeq(dds_lrt, test = "LRT", reduced = ~ pid)

  saveRDS(dds_lrt, dds_lrt_cache_file)
  message("Gespeichert: ", dds_lrt_cache_file)
}

# 2) LRT-Results laden oder berechnen (nur 1 Results-Objekt, kein contrast!)
run_or_load_rds <- function(rds_path, compute_expr, csv_path = NULL) {
  if (file.exists(rds_path)) {
    message("Lade: ", rds_path)
    return(readRDS(rds_path))
  }
  message("Berechne & speichere: ", rds_path)
  out <- eval(compute_expr)
  saveRDS(out, rds_path)
  if (!is.null(csv_path)) {
    write.csv(as.data.frame(out), csv_path)
    message("Zus√§tzlich als CSV exportiert: ", csv_path)
  }
  out
}

res_lrt <- run_or_load_rds(
  rds_path = res_lrt_rds,
  csv_path = res_lrt_csv,
  compute_expr = quote(results(dds_lrt, alpha = 0.05))
)

summary(res_lrt)

```
